{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2: Classic recognition\n",
    "\n",
    "Questions 2–7 are coding questions. You will implement different image representations and train linear classifiers with them. You will start with more flexible image representations and progressively move onto more rigid representations. Questions 8-10 are written questions to be answered in the PDF. You will report and reflect on the results, analyze the pros/cons of each representation and discuss possible improvements. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, pickle\n",
    "import numpy as np\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from utils import get_CIFAR10_data, train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "#### We provide you with a linear (softmax) classifier, as well as code to load the CIFAR-10 dataset.\n",
    "\n",
    "The CIFAR-10 dataset consists of 60,000 32x32 colour images in 10 classes, with 6,000 images per class. There are 50,000 training images and 10,000 test images. See the dataset website for more details: https://www.cs.toronto.edu/~kriz/cifar.html.\n",
    "\n",
    "**Do this:** Download the dataset from this link (https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz), place it where you want, and unzip it. Then try running the below code to see if you can load the dataset. Change `cifar10_dir` to your data path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "meta = pickle.load(open('data/cifar-10-batches-py/batches.meta', 'rb'), encoding='bytes')\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize a sample image\n",
    "i = 0\n",
    "label = y_train[i]\n",
    "class_name = meta[b'label_names'][label]\n",
    "plt.imshow(np.uint8(X_train[i])); plt.axis('off')\n",
    "plt.title('Label: {} ({})'.format(label, class_name)); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 2. Color features (5 points)\n",
    "\n",
    "First, we are going to explore using average color features to train a classifier. For each RGB color channel, average the pixel intensities. So a 32x32x3 image will be represented in a 1x3 vector.\n",
    "\n",
    "**Do this:** Implement the average color feature. Then train a classifier. Tune the regularization strength to train a good classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Compute average color features\n",
    "X_train = # YOUR CODE\n",
    "X_val = # YOUR CODE\n",
    "X_test = # YOUR CODE\n",
    "\n",
    "# TODO: Add bias dimension and transform into columns\n",
    "X_train = # YOUR CODE\n",
    "X_val = # YOUR CODE\n",
    "X_test = # YOUR CODE\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO : Define regularization strengths\n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train, y_train, X_val, y_val, X_test, y_test, regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 3. Bag of SIFT features (15 points)\n",
    "\n",
    "Bag of words models are a popular technique for image classification inspired by models used in natural language processing. The model ignores or downplays word arrangement (spatial information in the image) and classifies based on a histogram of the frequency of visual words. The visual word \"vocabulary\" is established by clustering a large corpus of local features. In this question, you will extract SIFT features from the training images. These result in a Nx128 dimensional matrix where N is the number of keypoints. After extracting SIFT features from all training images, we can use the K-means clustering algorithm to cluster these features into K clusters each represented by a 128-dimensional centroid. Now we have a bag of visual words (clusters) and can represent each image as a histogram of SIFT features assigned to these clusters. Specifically, each image will be represented as a K-dimensional histogram. Using these representations, you can train a classifier as before.\n",
    "\n",
    "**Do this**: Extract SIFT features. Do K-means clustering of the training images' SIFT features. Construct a histogram representation of the images and train a classifier. Specifically, implement `extract_sift()` in `features.py`. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example for extracting SIFT features\n",
    "\n",
    "Check out OpenCV's tutorial on extracting SIFT features: https://docs.opencv.org/3.4/da/df5/tutorial_py_sift_intro.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in the image\n",
    "img = cv2.imread('table.jpeg')\n",
    "\n",
    "# Convert to greyscale\n",
    "gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# Create a SIFT feature extractor\n",
    "sift = cv2.SIFT_create() # or cv2.xfeatures2d.SIFT_create()\n",
    "\n",
    "# Detect features from the image\n",
    "keypoints, descriptors = sift.detectAndCompute(gray, None)\n",
    "\n",
    "# Draw and visualize the detected keypoints on the image\n",
    "sift_image = cv2.drawKeypoints(gray, keypoints, img, flags=cv2.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.imshow(sift_image)\n",
    "plt.xticks([]), plt.yticks([])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Your work starts here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write a function extract sift featuress\n",
    "from features import extract_sift_for_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define step_size (sampling density) for sampling keypoints in a grid.\n",
    "step_size = 999\n",
    "\n",
    "# TODO: Extract dense SIFT features\n",
    "X_train_features = # YOUR CODE\n",
    "X_train_features_flattened = # YOUR CODE\n",
    "\n",
    "X_val_features = # YOUR CODE\n",
    "X_val_features_flattened = # YOUR CODE\n",
    "\n",
    "X_test_features = # YOUR CODE\n",
    "X_test_features_flattened = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Use your kmeans implementation from part 1 to cluster \n",
    "# the extracted SIFT features and build a visual vocabulary\n",
    "from kmeans import kmeans\n",
    "\n",
    "K = 999\n",
    "niter = 999\n",
    "labels_train, centroids = kmeans(X_train_features_flattened, K, niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Form histograms for images\n",
    "train_hist = # YOUR CODE\n",
    "val_hist = # YOUR CODE\n",
    "test_hist = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add bias dimension and transform into columns\n",
    "X_train = # YOUR CODE\n",
    "X_val = # YOUR CODE\n",
    "X_test = # YOUR CODE\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define regularization strengths\n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train, y_train, X_val, y_val, X_test, y_test, regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 4. SPM representation (15 points)\n",
    "\n",
    "One drawback of the bag-of-words approach is that it discards spatial information. Hence, we will now try encoding spatial information using Spatial Pyramid Matching (SPM) proposed in Lazebnik et al. 2006. At a high level, SPM works by breaking up an image into different regions and computing the SIFT descriptor at each region, forming a histogram of visual words in each region, and then concatenatating them into a single 1D vector representation.\n",
    "\n",
    "**Do this**: Construct a SPM representation of the images and train a classifier. Specifically, implement `spatial_pyramid_matching()` in `features.py`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define parameters {L, K, niter}\n",
    "L = 2 # Number of levels in SPM\n",
    "K = 16\n",
    "niter = 20\n",
    "\n",
    "# TODO: Extract SIFT features\n",
    "X_train_features = # YOUR CODE\n",
    "X_train_features_flattened = # YOUR CODE\n",
    "X_val_features = # YOUR CODE\n",
    "X_test_features = # YOUR CODE\n",
    "\n",
    "# TODO: Use your kmeans implementation from part 1 \n",
    "# to cluster the extracted SIFT features and build a visual vocabulary\n",
    "from kmeans import kmeans\n",
    "_, centroids = kmeans(X_train_features_flattened, K, niter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Write code to do SPM\n",
    "from features import spatial_pyramid_matching"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Get a SPM representation of the extracted SIFT features\n",
    "# Note: This may take some time to run\n",
    "X_train_spm = [spatial_pyramid_matching(L, \n",
    "                                        X_train_features[i].reshape((32, 32, 128)), \n",
    "                                        centroids) \n",
    "               for i in range(len(X_train))]\n",
    "\n",
    "X_val_spm = [spatial_pyramid_matching(L,\n",
    "                                      X_val_features[i].reshape((32, 32, 128)), \n",
    "                                      centroids) \n",
    "             for i in range(len(X_val))]\n",
    "\n",
    "X_test_spm = [spatial_pyramid_matching(L,\n",
    "                                       X_test_features[i].reshape((32, 32, 128)),\n",
    "                                       centroids)  \n",
    "              for i in range(len(X_test))]\n",
    "\n",
    "X_train_spm = np.array(X_train_spm)\n",
    "X_val_spm = np.array(X_val_spm)\n",
    "X_test_spm = np.array(X_test_spm)\n",
    "print(X_train_spm.shape, X_val_spm.shape, X_test_spm.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add bias dimension and transform into columns\n",
    "X_train = # YOUR CODE\n",
    "X_val = # YOUR CODE\n",
    "X_test = # YOUR CODE\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define regularization strengths\n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train, y_train, X_val, y_val, X_test, y_test, regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 5. Histogram of Oriented Gradients (10 points)\n",
    "\n",
    "Rather than extracting local SIFT features, we can compute a global histogram of oriented gradients (HOG) image descriptor. \n",
    "\n",
    "**Do this**: Implement `get_differential_filter()` and `filter_image()` in `features.py`. Then compute HOG descriptors and train a classifier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Implement get_differential_filter() and filter_image()\n",
    "# Note: extract_hog() will make use of these two functions.\n",
    "from features import extract_hog"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define parameters\n",
    "cell_size = 2 # Start with 2 or 4, but feel free to try other parameters\n",
    "block_size = 2 # Start with 2, but feel free to try other parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try running your code on a single image\n",
    "img = X_train[0]\n",
    "img = cv2.cvtColor(np.uint8(img), cv2.COLOR_BGR2GRAY)\n",
    "hog = extract_hog(img, cell_size=cell_size, block_size=block_size, plot=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Build HOG representations\n",
    "# Note: This may take some time to run\n",
    "X_train_hog = # YOUR CODE\n",
    "X_val_hog = # YOUR CODE\n",
    "X_test_hog = # YOUR CODE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Add bias dimension and transform into columns\n",
    "X_train = # YOUR CODE\n",
    "X_val = # YOUR CODE\n",
    "X_test = # YOUR CODE\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define regularization strengths\n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train, y_train, X_val, y_val, X_test, y_test, regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 6. Pixels (5 points)\n",
    "\n",
    "Finally, let's use the pixels themselves to train a classifier. That is, just reshape a 32x32x3 image into a 32x32x3=3072 vector.\n",
    "\n",
    "**Do this:** Process the images and train a classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freshly load the data\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = get_CIFAR10_data(cifar10_dir='data/cifar-10-batches-py',\n",
    "                                                                  num_training=4500, \n",
    "                                                                  num_validation=500, \n",
    "                                                                  num_test=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Reshape the image data into rows\n",
    "X_train = # YOUR CODE\n",
    "X_val = # YOUR CODE\n",
    "X_test = # YOUR CODE\n",
    "    \n",
    "# TODO: Normalize the data by subtracting the mean training image from all images\n",
    "    \n",
    "# TODO: Add bias dimension and transform into columns\n",
    "X_train = # YOUR CODE\n",
    "X_val = # YOUR CODE\n",
    "X_test = # YOUR CODE\n",
    "\n",
    "print('Train data shape: ', X_train.shape)\n",
    "print('Train labels shape: ', y_train.shape)\n",
    "print('Validation data shape: ', X_val.shape)\n",
    "print('Validation labels shape: ', y_val.shape)\n",
    "print('Test data shape: ', X_test.shape)\n",
    "print('Test labels shape: ', y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Define regularization strengths\n",
    "regularization_strengths = [1e3]\n",
    "\n",
    "# Train a classifier\n",
    "best_color = train(X_train, y_train, X_val, y_val, X_test, y_test, regularization_strengths)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 7. Results (10 points)\n",
    "\n",
    "**Do this**: \n",
    "\n",
    "7-a. Create a table of the five models' achieved accuracy, best hyperparameter, and runtime. (6 points)\n",
    "\n",
    "7-b. Briefly describe your results in a few sentences. Feel free to share your experience and highlight any interesting observations (e.g., you had to do more hyperparameter tuning for some than others). (4 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 8. Analysis (10 points)\n",
    "\n",
    "**Do this**: Create a confusion matrix for each of the five models. Feel free to use existing implementations such as [sklearn.metrics.confusion_matrix](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) but make sure to interpret some subset of the results and demonstrate that you understand what the values in the confusion matrices mean. Do the confusion matrices reveal any interesting insights (e.g., truck is always misclassified as automobile)? For each of the 10 classes, which model works best? Describe any hypotheses you have on the results. One or two paragraphs would be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 9. Improvement (9 points)\n",
    "\n",
    "**Do this**: Identify one shortcoming of one or few of the systems you've worked with. Name an improvement you can implement to improve the system(s). You don't have to actually implement your proposed improvement, but describe exactly how you could go about implementing it and what pitfalls you might anticipate. What would be the pros and cons of this intervention? One or two paragraphs would be sufficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Question 10. What to Use (6 points)\n",
    "\n",
    "So far we explored how different features work for 10-way image classification.\n",
    "\n",
    "**Do this**: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9-a. For the task of **object detection**, which features do you think would work best? Describe your reasons in a few senteces. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('objectdetection.jpeg')\n",
    "plt.figure(figsize=(10, 10)); plt.title('Object detection')\n",
    "plt.imshow(img); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9-b. For the task of **face detection**, which features do you think would work best? Describe your reasons in a few senteces. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('facedetection.png')\n",
    "plt.figure(figsize=(10, 10)); plt.title('Face detection')\n",
    "plt.imshow(img); plt.axis('off'); plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "9-c. For the task of **scene classification**, which features do you think would work best? Describe your reasons in a few senteces. (2 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = cv2.imread('sceneclassification.jpeg')\n",
    "plt.figure(figsize=(10, 10)); plt.title('Scene classification')\n",
    "plt.imshow(img); plt.axis('off'); plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
